import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
from tqdm import tqdm
import re
import os
from datetime import datetime


headers = {"User-Agent": "Mozilla/5.0"}
script_dir = os.path.dirname(os.path.abspath(__file__))
output_dir = os.path.join(script_dir, "scraped-data")
os.makedirs(output_dir, exist_ok=True)
checkpoint_path = os.path.join(output_dir, "done.log")
listing_base = "https://guland.vn"

province_slugs = {
    "soc-trang": "S√≥c TrƒÉng",
    "ha-noi": "H√† N·ªôi",
    "ha-giang": "H√† Giang",
    "cao-bang": "Cao B·∫±ng",
    "bac-kan": "B·∫Øc K·∫°n",
    "tuyen-quang": "Tuy√™n Quang",
    "lao-cai": "L√†o Cai",
    "dien-bien": "ƒêi·ªán Bi√™n",
    "lai-chau": "Lai Ch√¢u",
    "son-la": "S∆°n La",
    "yen-bai": "Y√™n B√°i",
    "hoa-binh": "H√≤a B√¨nh",
    "thai-nguyen": "Th√°i Nguy√™n",
    "lang-son": "L·∫°ng S∆°n",
    "quang-ninh": "Qu·∫£ng Ninh",
    "bac-giang": "B·∫Øc Giang",
    "phu-tho": "Ph√∫ Th·ªç",
    "vinh-phuc": "Vƒ©nh Ph√∫c",
    "bac-ninh": "B·∫Øc Ninh",
    "hai-duong": "H·∫£i D∆∞∆°ng",
    "hai-phong": "H·∫£i Ph√≤ng",
    "hung-yen": "H∆∞ng Y√™n",
    "thai-binh": "Th√°i B√¨nh",
    "ha-nam": "H√† Nam",
    "nam-dinh": "Nam ƒê·ªãnh",
    "ninh-binh": "Ninh B√¨nh",
    "thanh-hoa": "Thanh H√≥a",
    "nghe-an": "Ngh·ªá An",
    "ha-tinh": "H√† Tƒ©nh",
    "quang-binh": "Qu·∫£ng B√¨nh",
    "quang-tri": "Qu·∫£ng Tr·ªã",
    "thua-thien-hue": "Th·ª´a Thi√™n Hu·∫ø",
    "da-nang": "ƒê√† N·∫µng",
    "quang-nam": "Qu·∫£ng Nam",
    "quang-ngai": "Qu·∫£ng Ng√£i",
    "binh-dinh": "B√¨nh ƒê·ªãnh",
    "phu-yen": "Ph√∫ Y√™n",
    "khanh-hoa": "Kh√°nh H√≤a",
    "ninh-thuan": "Ninh Thu·∫≠n",
    "binh-thuan": "B√¨nh Thu·∫≠n",
    "kon-tum": "Kon Tum",
    "gia-lai": "Gia Lai",
    "dak-lak": "ƒê·∫Øk L·∫Øk",
    "dak-nong": "ƒê·∫Øk N√¥ng",
    "lam-dong": "L√¢m ƒê·ªìng",
    "binh-phuoc": "B√¨nh Ph∆∞·ªõc",
    "tay-ninh": "T√¢y Ninh",
    "binh-duong": "B√¨nh D∆∞∆°ng",
    "dong-nai": "ƒê·ªìng Nai",
    "ba-ria-vung-tau": "B√† R·ªãa - V≈©ng T√†u",
    "tp-ho-chi-minh": "TP. H·ªì Ch√≠ Minh",
    "long-an": "Long An",
    "tien-giang": "Ti·ªÅn Giang",
    "ben-tre": "B·∫øn Tre",
    "tra-vinh": "Tr√† Vinh",
    "vinh-long": "Vƒ©nh Long",
    "dong-thap": "ƒê·ªìng Th√°p",
    "an-giang": "An Giang",
    "kien-giang": "Ki√™n Giang",
    "can-tho": "C·∫ßn Th∆°",
    "hau-giang": "H·∫≠u Giang",
    "bac-lieu": "B·∫°c Li√™u",
    "ca-mau": "C√† Mau"
}

property_types = [
    "nha-mat-pho-mat-tien", "dat-tho-cu", "can-ho-chung-cu",
    "kho-nha-xuong", "van-phong", "phong-tro", "khach-san"
]

for province in province_slugs:
    for prop_type in property_types:
        try:
            checkpoint_key = f"{province}|{prop_type}"
            if os.path.exists(checkpoint_path) and checkpoint_key in open(checkpoint_path).read():
                print(f"‚è© Skipping {checkpoint_key}, already scraped.")
                continue

            print(f"\nüåç Scraping {province} - {prop_type}")
            page = 1
            all_data = []
            while True:
                print(f"\nüîé Page {page}...")
                url = f"https://guland.vn/mua-ban-{prop_type}-{province}?page={page}"
                response = requests.get(url, headers=headers)
                if response.status_code != 200:
                    print(f"‚ùå Failed at page {page}")
                    break

                soup = BeautifulSoup(response.text, "html.parser")
                listings = soup.select(".l-sdb-list__single")

                if not listings:
                    print("‚úÖ No more listings found.")
                    break

                print(f"üì¶ {len(listings)} listings on page {page}")

                for item in tqdm(listings, desc=f"üîÑ {province}-{prop_type}-p{page}", unit="listing"):
                    try:
                        detail_url = item.select_one(".c-sdb-card__tle a")["href"]
                        full_url = detail_url if "http" in detail_url else listing_base + detail_url
                        detail_resp = requests.get(full_url, headers=headers)
                        detail_soup = BeautifulSoup(detail_resp.text, "html.parser")

                        listing_id = "N/A"
                        for span in detail_soup.select(".dtl-stl__row span"):
                            if "M√£ tin" in span.get_text():
                                b = span.find("b")
                                if b:
                                    listing_id = b.get_text(strip=True)
                                break

                        title_tag = detail_soup.select_one(".dtl-tle")
                        vip_tag = title_tag.select_one(".vrf-bdg")
                        is_vip = bool(vip_tag)
                        if vip_tag:
                            vip_tag.extract()
                        title = title_tag.get_text(strip=True)

                        price = detail_soup.select_one(".dtl-prc__ttl").get_text(strip=True)
                        area = detail_soup.select_one(".dtl-prc__dtc").get_text(strip=True)

                        location = detail_soup.select_one(".dtl-stl__row > span")
                        location = location.get_text(strip=True) if location else "N/A"

                        updated_time = "N/A"
                        for span in detail_soup.select(".dtl-stl__row span"):
                            text = span.get_text()
                            if "C·∫≠p nh·∫≠t" in text:
                                updated_time = text.replace("C·∫≠p nh·∫≠t", "").strip()

                        def get_detail_value(label):
                            tag = detail_soup.find("div", class_="s-dtl-inf__lbl", string=lambda x: x and label in x)
                            return tag.find_next_sibling("div").get_text(strip=True) if tag else "N/A"

                        property_type_label = get_detail_value("Lo·∫°i BƒêS")
                        width = get_detail_value("Chi·ªÅu ngang")
                        length = get_detail_value("Chi·ªÅu d√†i")
                        bedrooms = get_detail_value("S·ªë ph√≤ng ng·ªß")
                        bathrooms = get_detail_value("S·ªë ph√≤ng t·∫Øm")
                        floors = get_detail_value("S·ªë t·∫ßng")
                        position = get_detail_value("V·ªã tr√≠")
                        direction = get_detail_value("H∆∞·ªõng c·ª≠a ch√≠nh")
                        alley_width = get_detail_value("ƒê∆∞·ªùng/h·∫ªm v√†o r·ªông")
                        road_type = get_detail_value("Lo·∫°i ƒë∆∞·ªùng")

                        description_tag = detail_soup.select_one(".dtl-inf__dsr")
                        description = description_tag.get_text(strip=True) if description_tag else "N/A"

                        gps_link = detail_soup.select_one("a.map-direction")
                        latitude = longitude = "N/A"
                        if gps_link:
                            href = gps_link.get("href", "")
                            match = re.search(r'query=([\d.]+),([\d.]+)', href)
                            if match:
                                latitude, longitude = match.group(1), match.group(2)

                        image_urls = []
                        for div in detail_soup.select(".media-thumb-wrap__inner"):
                            style = div.get("style", "")
                            match = re.search(r"url\('([^']+)'\)", style)
                            if match:
                                url = match.group(1)
                                if not url.endswith("map-icon.jpg"):
                                    image_urls.append(url)

                        images = "; ".join(image_urls) if image_urls else "N/A"

                        avatar_url = "0"
                        agent_role = "N/A"
                        agent_name = "N/A"
                        agent_listing_count = "N/A"

                        avatar_tag = detail_soup.select_one(".dtl-aut__avt img")
                        if avatar_tag:
                            src = avatar_tag.get("src", "")
                            if "profile.png" not in src:
                                avatar_url = src

                        role_tag = detail_soup.select_one(".dtl-aut__rol")
                        if role_tag:
                            agent_role = role_tag.get_text(strip=True)

                        name_tag = detail_soup.select_one(".dtl-aut__tle")
                        if name_tag:
                            agent_name = name_tag.get_text(strip=True)

                        listing_count_tag = detail_soup.select_one(".dtl-aut__stl")
                        if listing_count_tag:
                            match = re.search(r'(\d+)', listing_count_tag.get_text())
                            if match:
                                agent_listing_count = match.group(1)

                        scraped_at = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

                        all_data.append([
                            title, price, area, location, listing_id, updated_time,
                            property_type_label, width, length, bedrooms, bathrooms, floors,
                            position, direction, alley_width, road_type,
                            description, full_url, latitude, longitude, is_vip, images,
                            avatar_url, agent_role, agent_name, agent_listing_count,
                            province, prop_type, scraped_at
                        ])
                        time.sleep(0.5)

                    except Exception as e:
                        print(f"‚ö†Ô∏è Listing error: {e}")
                        continue

                page += 1
                time.sleep(2)

            # Save per province
            outpath = os.path.join(output_dir, f"{province}.csv")
            df = pd.DataFrame(all_data, columns=[
                "Title", "Price", "Area", "Location", "Listing ID", "Last Updated",
                "Property Type", "Width", "Length", "Bedrooms", "Bathrooms", "Floors",
                "Position", "Direction", "Alley Width", "Road Type",
                "Description", "URL", "Latitude", "Longitude", "VIP Account", "Images",
                "Avatar", "Agent Role", "Agent Name", "Agent Listing Count",
                "Province", "Property Type Slug", "Scraped At"
            ])

            if os.path.exists(outpath):
                df.to_csv(outpath, mode='a', header=False, index=False, encoding='utf-8-sig')
            else:
                df.to_csv(outpath, index=False, encoding='utf-8-sig')

            with open(checkpoint_path, "a") as log:
                log.write(f"{checkpoint_key}\n")

            print(f"‚úÖ Saved {len(all_data)} listings for {province}-{prop_type}")

        except Exception as err:
            with open(os.path.join(output_dir, "failed.log"), "a") as fail:
                fail.write(f"{province}|{prop_type} - {err}\n")
            print(f"‚ùå Failed {province}|{prop_type}: {err}")
            continue

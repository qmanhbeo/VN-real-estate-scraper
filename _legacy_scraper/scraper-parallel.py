import requests
from bs4 import BeautifulSoup
import pandas as pd
import time, re, os
from tqdm import tqdm
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed

# ========================
# CONFIGURATION
# ========================
HEADERS = {"User-Agent": "Mozilla/5.0"}
MAX_WORKERS = 16   # number of parallel threads for detail pages
PAGE_SLEEP = 2     # pause between listing pages (seconds)
DETAIL_TIMEOUT = 15  # timeout for detail requests (seconds)
CUTOFF_COUNT = 45  # stop paginating if fewer listings than this on a page
# ========================

script_dir = os.path.dirname(os.path.abspath(__file__))
output_dir = os.path.join(script_dir, "scraped-data")
os.makedirs(output_dir, exist_ok=True)
checkpoint_dir = os.path.join(output_dir, "checkpoint")
os.makedirs(checkpoint_dir, exist_ok=True)
checkpoint_path = os.path.join(checkpoint_dir, "done.log")
listing_base = "https://guland.vn"

session = requests.Session()
session.headers.update(HEADERS)

province_slugs = {
    "soc-trang": "S√≥c TrƒÉng",
    "ha-noi": "H√† N·ªôi",
    "ha-giang": "H√† Giang",
    "cao-bang": "Cao B·∫±ng",
    "bac-kan": "B·∫Øc K·∫°n",
    "tuyen-quang": "Tuy√™n Quang",
    "lao-cai": "L√†o Cai",
    "dien-bien": "ƒêi·ªán Bi√™n",
    "lai-chau": "Lai Ch√¢u",
    "son-la": "S∆°n La",
    "yen-bai": "Y√™n B√°i",
    "hoa-binh": "H√≤a B√¨nh",
    "thai-nguyen": "Th√°i Nguy√™n",
    "lang-son": "L·∫°ng S∆°n",
    "quang-ninh": "Qu·∫£ng Ninh",
    "bac-giang": "B·∫Øc Giang",
    "phu-tho": "Ph√∫ Th·ªç",
    "vinh-phuc": "Vƒ©nh Ph√∫c",
    "bac-ninh": "B·∫Øc Ninh",
    "hai-duong": "H·∫£i D∆∞∆°ng",
    "hai-phong": "H·∫£i Ph√≤ng",
    "hung-yen": "H∆∞ng Y√™n",
    "thai-binh": "Th√°i B√¨nh",
    "ha-nam": "H√† Nam",
    "nam-dinh": "Nam ƒê·ªãnh",
    "ninh-binh": "Ninh B√¨nh",
    "thanh-hoa": "Thanh H√≥a",
    "nghe-an": "Ngh·ªá An",
    "ha-tinh": "H√† Tƒ©nh",
    "quang-binh": "Qu·∫£ng B√¨nh",
    "quang-tri": "Qu·∫£ng Tr·ªã",
    "thua-thien-hue": "Th·ª´a Thi√™n Hu·∫ø",
    "da-nang": "ƒê√† N·∫µng",
    "quang-nam": "Qu·∫£ng Nam",
    "quang-ngai": "Qu·∫£ng Ng√£i",
    "binh-dinh": "B√¨nh ƒê·ªãnh",
    "phu-yen": "Ph√∫ Y√™n",
    "khanh-hoa": "Kh√°nh H√≤a",
    "ninh-thuan": "Ninh Thu·∫≠n",
    "binh-thuan": "B√¨nh Thu·∫≠n",
    "kon-tum": "Kon Tum",
    "gia-lai": "Gia Lai",
    "dak-lak": "ƒê·∫Øk L·∫Øk",
    "dak-nong": "ƒê·∫Øk N√¥ng",
    "lam-dong": "L√¢m ƒê·ªìng",
    "binh-phuoc": "B√¨nh Ph∆∞·ªõc",
    "tay-ninh": "T√¢y Ninh",
    "binh-duong": "B√¨nh D∆∞∆°ng",
    "dong-nai": "ƒê·ªìng Nai",
    "ba-ria-vung-tau": "B√† R·ªãa - V≈©ng T√†u",
    "tp-ho-chi-minh": "TP. H·ªì Ch√≠ Minh",
    "long-an": "Long An",
    "tien-giang": "Ti·ªÅn Giang",
    "ben-tre": "B·∫øn Tre",
    "tra-vinh": "Tr√† Vinh",
    "vinh-long": "Vƒ©nh Long",
    "dong-thap": "ƒê·ªìng Th√°p",
    "an-giang": "An Giang",
    "kien-giang": "Ki√™n Giang",
    "can-tho": "C·∫ßn Th∆°",
    "hau-giang": "H·∫≠u Giang",
    "bac-lieu": "B·∫°c Li√™u",
    "ca-mau": "C√† Mau"
}

property_types = [
    "nha-mat-pho-mat-tien", "dat-tho-cu", "can-ho-chung-cu",
    "kho-nha-xuong", "van-phong", "phong-tro", "khach-san"
]

# ----------------------------
# DETAIL PAGE PARSER (worker)
# ----------------------------
def parse_detail(full_url, province, prop_type):
    try:
        resp = session.get(full_url, timeout=DETAIL_TIMEOUT)
        if resp.status_code != 200:
            return None
        soup = BeautifulSoup(resp.text, "html.parser")

        listing_id = "N/A"
        for span in soup.select(".dtl-stl__row span"):
            if "M√£ tin" in span.get_text():
                b = span.find("b")
                if b:
                    listing_id = b.get_text(strip=True)
                break

        title_tag = soup.select_one(".dtl-tle")
        if title_tag:
            vip_tag = title_tag.select_one(".vrf-bdg")
            is_vip = bool(vip_tag)
            if vip_tag:
                vip_tag.extract()
            title = title_tag.get_text(strip=True)
        else:
            is_vip, title = False, "N/A"

        def safe_text(sel):
            tag = soup.select_one(sel)
            return tag.get_text(strip=True) if tag else "N/A"

        price = safe_text(".dtl-prc__ttl")
        area = safe_text(".dtl-prc__dtc")
        location = safe_text(".dtl-stl__row > span")

        updated_time = "N/A"
        for span in soup.select(".dtl-stl__row span"):
            text = span.get_text()
            if "C·∫≠p nh·∫≠t" in text:
                updated_time = text.replace("C·∫≠p nh·∫≠t", "").strip()

        def get_detail_value(label):
            tag = soup.find("div", class_="s-dtl-inf__lbl", string=lambda x: x and label in x)
            return tag.find_next_sibling("div").get_text(strip=True) if tag else "N/A"

        property_type_label = get_detail_value("Lo·∫°i BƒêS")
        width = get_detail_value("Chi·ªÅu ngang")
        length = get_detail_value("Chi·ªÅu d√†i")
        bedrooms = get_detail_value("S·ªë ph√≤ng ng·ªß")
        bathrooms = get_detail_value("S·ªë ph√≤ng t·∫Øm")
        floors = get_detail_value("S·ªë t·∫ßng")
        position = get_detail_value("V·ªã tr√≠")
        direction = get_detail_value("H∆∞·ªõng c·ª≠a ch√≠nh")
        alley_width = get_detail_value("ƒê∆∞·ªùng/h·∫ªm v√†o r·ªông")
        road_type = get_detail_value("Lo·∫°i ƒë∆∞·ªùng")

        description = safe_text(".dtl-inf__dsr")

        gps_link = soup.select_one("a.map-direction")
        latitude = longitude = "N/A"
        if gps_link:
            href = gps_link.get("href", "")
            match = re.search(r'query=([\d.]+),([\d.]+)', href)
            if match:
                latitude, longitude = match.group(1), match.group(2)

        image_urls = []
        for div in soup.select(".media-thumb-wrap__inner"):
            style = div.get("style", "")
            match = re.search(r"url\('([^']+)'\)", style)
            if match:
                url = match.group(1)
                if not url.endswith("map-icon.jpg"):
                    image_urls.append(url)
        images = "; ".join(image_urls) if image_urls else "N/A"

        avatar_url, agent_role, agent_name, agent_listing_count = "0", "N/A", "N/A", "N/A"
        avatar_tag = soup.select_one(".dtl-aut__avt img")
        if avatar_tag and "profile.png" not in avatar_tag.get("src", ""):
            avatar_url = avatar_tag["src"]

        role_tag = soup.select_one(".dtl-aut__rol")
        if role_tag: agent_role = role_tag.get_text(strip=True)
        name_tag = soup.select_one(".dtl-aut__tle")
        if name_tag: agent_name = name_tag.get_text(strip=True)
        listing_count_tag = soup.select_one(".dtl-aut__stl")
        if listing_count_tag:
            m = re.search(r'(\d+)', listing_count_tag.get_text())
            if m: agent_listing_count = m.group(1)

        scraped_at = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        return [
            title, price, area, location, listing_id, updated_time,
            property_type_label, width, length, bedrooms, bathrooms, floors,
            position, direction, alley_width, road_type,
            description, full_url, latitude, longitude, is_vip, images,
            avatar_url, agent_role, agent_name, agent_listing_count,
            province, prop_type, scraped_at
        ]
    except Exception:
        return None

# ----------------------------
# PARALLEL FETCH FOR A PAGE
# ----------------------------
def fetch_page_details(listings, province, prop_type, max_workers=MAX_WORKERS):
    urls = []
    for item in listings:
        link = item.select_one(".c-sdb-card__tle a")
        if link and link.get("href"):
            href = link["href"]
            urls.append(href if "http" in href else listing_base + href)

    results = []
    with ThreadPoolExecutor(max_workers=max_workers) as ex:
        futures = [ex.submit(parse_detail, url, province, prop_type) for url in urls]
        for fut in as_completed(futures):
            res = fut.result()
            if res: results.append(res)
    return results

# ----------------------------
# MAIN LOOP
# ----------------------------
for province in province_slugs:
    for prop_type in property_types:
        try:
            checkpoint_key = f"{province}|{prop_type}"
            if os.path.exists(checkpoint_path) and checkpoint_key in open(checkpoint_path).read():
                print(f"‚è© Skipping {checkpoint_key}, already scraped.")
                continue

            print(f"\nüåç Scraping {province} - {prop_type}")
            page = 1
            all_data = []
            while True:
                print(f"\nüîé Page {page}...")
                url = f"https://guland.vn/mua-ban-{prop_type}-{province}?page={page}"
                response = session.get(url, timeout=DETAIL_TIMEOUT)
                if response.status_code != 200:
                    print(f"‚ùå Failed at page {page}")
                    break

                soup = BeautifulSoup(response.text, "html.parser")
                listings = soup.select(".l-sdb-list__single")

                if not listings:
                    print("‚úÖ No more listings found.")
                    break

                print(f"üì¶ {len(listings)} listings on page {page}")

                # cutoff condition
                last_page = False
                if len(listings) < CUTOFF_COUNT:
                    print(f"‚ÑπÔ∏è Less than {CUTOFF_COUNT} listings ‚Üí scrape this page and stop pagination after.")
                    last_page = True

                # parallel scrape detail pages
                page_results = fetch_page_details(listings, province, prop_type, max_workers=MAX_WORKERS)
                all_data.extend(page_results)

                page += 1
                if last_page:
                    break
                time.sleep(PAGE_SLEEP)

            # save per province
            outpath = os.path.join(output_dir, f"{province}.csv")
            df = pd.DataFrame(all_data, columns=[
                "Title", "Price", "Area", "Location", "Listing ID", "Last Updated",
                "Property Type", "Width", "Length", "Bedrooms", "Bathrooms", "Floors",
                "Position", "Direction", "Alley Width", "Road Type",
                "Description", "URL", "Latitude", "Longitude", "VIP Account", "Images",
                "Avatar", "Agent Role", "Agent Name", "Agent Listing Count",
                "Province", "Property Type Slug", "Scraped At"
            ])

            if os.path.exists(outpath):
                df.to_csv(outpath, mode='a', header=False, index=False, encoding='utf-8-sig')
            else:
                df.to_csv(outpath, index=False, encoding='utf-8-sig')

            with open(checkpoint_path, "a") as log:
                log.write(f"{checkpoint_key}\n")

            print(f"‚úÖ Saved {len(all_data)} listings for {province}-{prop_type}")

        except Exception as err:
            with open(os.path.join(output_dir, "failed.log"), "a") as fail:
                fail.write(f"{province}|{prop_type} - {err}\n")
            print(f"‚ùå Failed {province}|{prop_type}: {err}")
            continue
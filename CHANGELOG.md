# ðŸ“œ Changelog

## 2025-09-12

### ðŸš€ Major Improvements
- Added **parallel scraping** with `scraper-parallel.py`
  - Uses multi-threading for detail pages â†’ reduces runtime from ~20h to just a fraction.
- Added **incremental CSV writing** with `scraper-parallel-incrementCSV.py`
  - Saves data page-by-page instead of at the end.
  - Introduced perâ€“listing ID checkpoints â†’ prevents duplicate entries and allows seamless resume if interrupted.
  - All checkpoint files are now stored neatly in `scraped-data/checkpoint/`.
- Added last page identification using count of listings -> Avoid endless pagination

### âœ… Outcomes
- Scraper runs **much faster** (parallel workers configurable via `MAX_WORKERS`).
- Data is **more reliable** thanks to incremental writes + ID-level checkpoints.
- Accidental duplicate listings across pages are automatically removed.
- Repo structure is cleaner with a dedicated `checkpoint/` directory.

### ðŸ’¡ Notes
- `scraper.py` (original single-threaded) is kept for stability/testing.
- `scraper-parallel.py` is fast but relies only on provinceâ€“property checkpoints.
- `scraper-parallel-incrementCSV.py` is the recommended production scraper.

---
